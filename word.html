<!DOCTYPE html>
<html>
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <link href='https://fonts.googleapis.com/css?family=Architects+Daughter' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/print.css" media="print">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <title>SPARK-n-SPELL by dominedo</title>
  </head>

  <body>
    <header>

      <div class="inner">

        <h1>SPARK-n-SPELL</h1>
        <h2>Context-based document correction in Apache SPARK</h2>
        
        <a href="https://github.com/dominedo/spark-n-spell" class="button"><small>View project on</small> GitHub</a>

        <nav>
          <ul>
            <li><a href='index.html'>Home</a></li>
            <li><a href='word.html'>Word-Level Checking</a></li>
            <li><a href='context.html'>Context-Level Checking</a></li>
            <li><a href='performance.html'>Performance</a></li>
          </ul>
        </nav>

      </div>

    </header>

    <div id="content-wrapper">
      <div class="inner clearfix">
        <section id="main-content">

<h3>
<a id="designer-templates" class="anchor" href="#designer-templates" aria-hidden="true"><span class="octicon octicon-link"></span></a>Word-Level Checking</h3>

<p>
There are many modern applications in which spelling correction algorithms play an important role, and these algorithms vary widely in complexity. The simplest ones are based on the concept of edit distance:  this is the number of edits it takes to turn one word into another.
</p>
<p>
For a given word, any single edit can be represented by the removal of one letter (deletions), the swapping of adjacent letters (transpositions), the changing of one letter to another (replacements), and the addition of a letter to the word (insertions). A conventional correction algorithm might generate all of these possible edits for a word to be checked, to be compared with entries in a dictionary.
</p>
<p>
In 2012, details of a Symmetric Delete Spelling Correction algorithm were published, that exploits the fact that the edit distance between two terms is symmetrical. That is, you can readily determine whether any two words differ by a single edit by considering only character deletions, and then matching those strings generated by deleting characters from both words. By checking only deletions rather than four types of edits, substantial performance improvements can be achieved.
</p>
<p>
For our project, we implemented this serial Symmetric Delete algorithm in Python, and used it as a baseline for a new implementation in Spark. This allowed us to determine whether the addition of parallelism would afford any additional benefits.</p>

<div style="text-align: center;">
<iframe width="420" height="315" src="https://www.youtube.com/embed/yTg7j9uu0ss" frameborder="0" allowfullscreen></iframe>
</div>

<h3>
<a id="designer-templates" class="anchor" href="#designer-templates" aria-hidden="true"><span class="octicon octicon-link"></span></a>Serial Python SYMSPELL Implementation</h3>
<ul>
<li><a href='https://github.com/dominedo/spark-n-spell/blob/master/symspell_python.py'>symspell_python.py</a></li>
</ul>
<p>
This program is a Python version of a spellchecker based on SymSpell, 
the Symmetric Delete spelling correction algorithm developed by Wolf Garbe 
and originally written in C#.
</p>
<p>
From the original SymSpell documentation:
</p>
<blockquote>
The Symmetric Delete spelling correction algorithm reduces the complexity 
 of edit candidate generation and dictionary lookup for a given Damerau-
 Levenshtein distance. It is six orders of magnitude faster and language 
 independent. Opposite to other algorithms only deletes are required, 
 no transposes + replaces + inserts. Transposes + replaces + inserts of the 
 input term are transformed into deletes of the dictionary term.
 Replaces and inserts are expensive and language dependent: 
 e.g. Chinese has 70,000 Unicode Han characters!
</blockquote>
<p>
For further information on SymSpell, please consult the original
<a href='http://blog.faroo.com/2012/06/07/improved-edit-distance-based-spelling-correction/' target='_blank'>documentation</a>.
</p>
<p>
The current version of this program will output all possible suggestions for
corrections up to an edit distance (configurable) of max_edit_distance = 3. 
With the exception of the use of a third-party method for calculating
Demerau-Levenshtein distance between two strings, we have largely followed 
the structure and spirit of the original SymSpell algorithm and have not 
introduced any major optimizations or improvements.
</p>
<h3>
<a id="designer-templates" class="anchor" href="#designer-templates" aria-hidden="true"><span class="octicon octicon-link"></span></a>Apache SPARK Implementations</h3>
<ul>
<li><a href='https://github.com/dominedo/spark-n-spell/blob/master/word_correct_spark.py'>word_correct_spark.py (for words)</a></li>
<li><a href='https://github.com/dominedo/spark-n-spell/blob/master/word_level_doc_correct.py'>word_level_doc_correct.py (for documents)</a></li>
</ul>
<p>
We experimented with parallelizing various aspects of the algorithm, including the generation of the dictionary, the generation of n-deletes (in both construction of the dictionary and for the input word), and the checking of multiple words in a document. The final combination for running both locally and on AWS is provided in the above-noted file.
</p>
<p>
Important note:  The current version will generate internally all possible correction for each word in the test document before choosing the best one. This allows us to compare the performance of earlier versions of the code, and feeds into the context-level document checking algorithm. This means that this code will run slowly for any moderately-sized test file. For faster Spark versions that correct documents, please consider running the context-based versions.
</p>
  <p>
    Please refer to the <a href='https://github.com/dominedo/spark-n-spell/blob/master/word_level_documentation.ipynb'>iPython notebook</a> for more details on our coding and testing process.
  </p>
        </section>

        <aside id="sidebar">
          <a href="https://github.com/dominedo/spark-n-spell/zipball/master" class="button">
            <small>Download</small>
            .zip file
          </a>
          <a href="https://github.com/dominedo/spark-n-spell/tarball/master" class="button">
            <small>Download</small>
            .tar.gz file
          </a>

          <p class="repo-owner"><a href="https://github.com/dominedo/spark-n-spell"></a> is maintained by <a href="https://github.com/dominedo">dominedo</a>.</p>

          <p>This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the Architect theme by <a href="https://twitter.com/jasonlong">Jason Long</a>.</p>
        </aside>
      </div>
    </div>

  
  </body>
</html>
